{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "context_length = 512\n",
    "batch_size = 4\n",
    "n_embed = 512  #Number of embedding dimensions\n",
    "n_layers = 12\n",
    "n_heads = 8\n",
    "vocab_size = 50304\n",
    "resume_training = False\n",
    "grad_accum_steps = 20\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model_parameters = dict( vocab_size = vocab_size, context_length = context_length, n_embed = n_embed, n_layers = n_layers,\n",
    "    n_heads = n_heads)\n",
    "\n",
    "class DataLoader:\n",
    "    def __init__(self, file_name, batch_size, context_length):\n",
    "      self.data = np.memmap(file_name, dtype=np.uint16, mode='r+')\n",
    "      self.n_examples = len(self.data)\n",
    "      self.current = 0\n",
    "      self.batch_size = batch_size\n",
    "      self.context_length = context_length\n",
    "      self.full_context = self.batch_size * self.context_length\n",
    "      self.n_valid_sequences = (self.n_examples - 1) // self.full_context\n",
    "      self.indexes = np.arange(0, self.n_valid_sequences * self.full_context, self.full_context)\n",
    "\n",
    "    def shuffle(self):\n",
    "      np.random.shuffle(self.indexes)\n",
    "\n",
    "    def get_batch(self):\n",
    "      if self.current == self.n_valid_sequences:\n",
    "        self.current = 0\n",
    "        self.shuffle()\n",
    "\n",
    "      idx = self.indexes[self.current]\n",
    "      x = torch.from_numpy((self.data[idx: idx + self.full_context]).astype(np.int64)).view(-1,self.context_length)\n",
    "      y = torch.from_numpy((self.data[idx+1 : idx + self.full_context+1]).astype(np.int64)).view(-1,self.context_length)\n",
    "      self.current += 1\n",
    "\n",
    "      x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
    "\n",
    "      return x,y\n",
    "\n",
    "config = MiniGPTConfig(**model_parameters)\n",
    "model = MiniGPT(config).to(device)\n",
    "train_loader = DataLoader(\"train.bin\", batch_size, context_length)\n",
    "val_loader = DataLoader(\"val.bin\", batch_size, context_length)\n",
    "\n",
    "model = torch.compile(model)\n",
    "checkpoints_dir = \"checkpoints\"\n",
    "os.makedirs(checkpoints_dir, exist_ok=True)\n",
    "\n",
    "if resume_training == True:\n",
    "   checkpoint_dir = os.path.join(checkpoints_dir,\"step_500.pt\")\n",
    "   state_dict = torch.load(checkpoint_dir, map_location= device)\n",
    "   model.load_state_dict(state_dict[\"model\"])\n",
    "   for optimizer, state in zip(optimizers, state_dict[\"optimizer_states\"]):\n",
    "        optimizer.load_state_dict(state)\n",
    "   loss = state_dict[\"val_loss\"]\n",
    "   step = state_dict[\"step\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "gradients = []\n",
    "\n",
    "for small_grad_step in range(grad_accum_steps):\n",
    "    xb, yb = train_loader.get_batch()  \n",
    "    xb, yb = xb.to(device), yb.to(device)\n",
    "\n",
    "    logits, loss = model(xb, yb) \n",
    "    loss.backward()\n",
    "\n",
    "    step_gradients = [param.grad.clone().detach() for param in model.parameters()]\n",
    "    gradients.append(step_gradients)\n",
    "    model.zero_grad(set_to_none=True)\n",
    "\n",
    "gradients = np.array([[g.cpu().numpy() for g in step] for step in gradients], dtype=object)\n",
    "mean_gradients = [np.mean([grad[step] for grad in gradients], axis=0) for step in range(len(gradients[0]))]\n",
    "\n",
    "deviations = []\n",
    "for step in range(grad_accum_steps):\n",
    "    step_deviation = [\n",
    "        grad[step] - mean_gradients[step]\n",
    "        for grad in gradients\n",
    "    ]\n",
    "    deviations.append(step_deviation)\n",
    "\n",
    "covariances = []\n",
    "for param_index in range(len(mean_gradients)):\n",
    "    deviations_matrix = np.stack([deviation[param_index] for deviation in deviations])\n",
    "    param_covariance = np.cov(deviations_matrix, rowvar=False)\n",
    "    covariances.append(param_covariance)\n",
    "\n",
    "noise_scales = []\n",
    "for param_index in range(len(covariances)):\n",
    "    param_covariance = covariances[param_index]\n",
    "    mean_gradient = mean_gradients[param_index]\n",
    "\n",
    "    trace = np.trace(param_covariance)\n",
    "    mean_grad_norm_squared = np.linalg.norm(mean_gradient) ** 2\n",
    "\n",
    "    if mean_grad_norm_squared > 0:\n",
    "        noise_scale = trace / mean_grad_norm_squared\n",
    "        noise_scales.append(noise_scale)\n",
    "\n",
    "total_noise_scale = sum(noise_scales)\n",
    "\n",
    "print(f\"Gradient Noise Scale: {total_noise_scale}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
